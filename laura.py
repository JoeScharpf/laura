# -*- coding: utf-8 -*-
"""laura.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IxPPX4s7Meuju4PLV0d-ysS6Di3O2gpp
"""

class search:
  def binary_search(self, lst, target):
    length = len(lst)
    left = 0
    right = length - 1
    while left <= right:
      mid = (left + right) // 2
      if lst[mid] == target:
        return mid
      elif lst[mid] > target:
        right = mid - 1
      elif lst[mid] < target:
        left = mid + 1
    return - 1

class solution:
  def valdi_parentheses(self, s):
    stack = []
    brackets = {')': '(', '}': '{', ']': '['}
    for i in s:
      if i in brackets:
        if stack and stack[-1] == brackets[i]:
          stack.pop()
        else:
          return False
      else:
        stack.append(i)
    return True if not stack else False

pip install open-clip-torch

!pip install git+https://github.com/openai/CLIP.git

from google.colab import files
uploaded = files.upload()
import torch
import clip
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"

model, preprocess = clip.load("ViT-B/32", device=device)

image_path = list(uploaded.keys())[0]
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)

text_prompts = ["a photo of a dog", "a photo of a cat", "a photo of a diagram"]
text_tokens = clip.tokenize(text_prompts).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text_tokens)

    logits_per_image, logits_per_text = model(image, text_tokens)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]

for label, prob in zip(text_prompts, probs):
    print(f"{label}: {prob*100:.2f}%")

pip install transformers datasets peft accelerate

from transformers import CLIPModel, CLIPProcessor

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.FEATURE_EXTRACTION,
    target_modules=["q_proj", "v_proj"],
)

model = get_peft_model(model, lora_config)

trainable_params = 0
frozen_params = 0

for name, param in model.named_parameters():
    if param.requires_grad:
        trainable_params += param.numel()
    else:
        frozen_params += param.numel()

total_params = trainable_params + frozen_params

print(f"Frozen weight count: {frozen_params:,}")
print(f"Fine-tune LoRA weights: {trainable_params:,}")
print(f"Total model parameters: {total_params:,}")
print(f"Percent of trainable params: {100 * trainable_params / total_params:.4f}%")

for name, module in model.named_modules():
    print(name)

pip install datasets transformers

import kagglehub

path = kagglehub.dataset_download("adityajn105/flickr8k")
print("Path to dataset files:", path)

import pandas as pd

captions_path = f"{path}/captions.txt"
df = pd.read_csv(captions_path)

filename = df.iloc[500]["image"]
caption = df.iloc[500]["caption"]

print("Image filename:", filename)
print("Caption:", caption)

from PIL import Image
import matplotlib.pyplot as plt

image_path = f"{path}/Images/{filename}"
image = Image.open(image_path)

plt.imshow(image)
plt.axis("off")
plt.title(caption)
plt.show()

from transformers import CLIPModel, CLIPProcessor
from peft import get_peft_model, LoraConfig, TaskType
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

base_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(base_model, lora_config)

model.print_trainable_parameters()

from torch.utils.data import Dataset

class CLIPFlickrDataset(Dataset):
    def __init__(self, csv_path, image_folder, processor):
        self.data = pd.read_csv(csv_path)
        self.image_folder = image_folder
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image = Image.open(f"{self.image_folder}/{row['image']}").convert("RGB")
        caption = row['caption']
        return {"image": image, "text": caption}

def collate_fn(batch):
    images = [item["image"] for item in batch]
    texts = [item["text"] for item in batch]

    encoded = processor(
        text=texts,
        images=images,
        return_tensors="pt",
        padding=True,
        truncation=True
    )

    return {
        "input_ids": encoded["input_ids"],
        "attention_mask": encoded["attention_mask"],
        "pixel_values": encoded["pixel_values"]
    }

from torch.utils.data import DataLoader

dataset = CLIPFlickrDataset(
    csv_path=f"{path}/captions.txt",
    image_folder=f"{path}/Images",
    processor=processor
)

dataloader = DataLoader(
    dataset,
    batch_size=8,
    shuffle=True,
    collate_fn=collate_fn
)

import torch.nn.functional as F

def clip_loss(logits_per_image, logits_per_text):
    labels = torch.arange(logits_per_image.size(0), device=logits_per_image.device)
    loss_i = F.cross_entropy(logits_per_image, labels)
    loss_t = F.cross_entropy(logits_per_text, labels)
    return (loss_i + loss_t) / 2

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=1e-4)
model.train()

model.print_trainable_parameters()

# recall@5

def recall_at_k(logits, k=5):
    targets = torch.arange(logits.size(0), device=logits.device)
    _, topk = logits.topk(k, dim=1)
    correct = topk.eq(targets.unsqueeze(1))
    return correct.any(dim=1).float().mean().item()


for epoch in range(1):
    total_loss = 0
    total_recall5 = 0

    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        pixel_values = batch["pixel_values"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values
        )

        logits_per_image = outputs.logits_per_image
        logits_per_text = outputs.logits_per_text
        loss = clip_loss(logits_per_image, logits_per_text)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()

        # ðŸ” Recall@5 computation
        r5_i2t = recall_at_k(logits_per_image, k=5)
        r5_t2i = recall_at_k(logits_per_text, k=5)
        batch_recall5 = (r5_i2t + r5_t2i) / 2
        total_recall5 += batch_recall5

        print(f"Batch loss: {loss.item():.4f} | Batch Recall@5: {batch_recall5 * 100:.2f}%")

    avg_loss = total_loss / len(dataloader)
    avg_recall5 = total_recall5 / len(dataloader)
    print(f"Epoch {epoch + 1} - Avg Loss: {avg_loss:.4f} | Avg Recall@5: {avg_recall5 * 100:.2f}%")

# recall@1

def recall_at_k(logits, k=1):  # now defaulting to Recall@1
    targets = torch.arange(logits.size(0), device=logits.device)
    _, topk = logits.topk(k, dim=1)
    correct = topk.eq(targets.unsqueeze(1))
    return correct.any(dim=1).float().mean().item()


for epoch in range(1):
    total_loss = 0
    total_recall1 = 0  # Track Recall@1 instead of @5

    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        pixel_values = batch["pixel_values"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values
        )

        logits_per_image = outputs.logits_per_image
        logits_per_text = outputs.logits_per_text
        loss = clip_loss(logits_per_image, logits_per_text)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()

        r1_i2t = recall_at_k(logits_per_image, k=1)
        r1_t2i = recall_at_k(logits_per_text, k=1)
        batch_recall1 = (r1_i2t + r1_t2i) / 2
        total_recall1 += batch_recall1

        print(f"Batch loss: {loss.item():.4f} | Batch Recall@1: {batch_recall1 * 100:.2f}%")

    avg_loss = total_loss / len(dataloader)
    avg_recall1 = total_recall1 / len(dataloader)
    print(f"Epoch {epoch + 1} - Avg Loss: {avg_loss:.4f} | Avg Recall@1: {avg_recall1 * 100:.2f}%")